Ygrid = TRUE)
UKgrid_tstbl <- extract_grid(type = "tsibble",
columns = "ND",
aggregate = "hourly",
na.rm = TRUE)
UKgrid_tbl <-extract_grid(type = "tbl",
columns = "ND",
aggregate = "hourly",
na.rm = TRUE)
library(xts)
UKgrid_df <- data.frame(time = zoo::index(UKgrid_xts), UKgrid=as.numeric(UKgrid_xts))
str(UKgrid_df)
library(lubridate)
UKgrid_df$hour <- hour(UKgrid_df$time)
UKgrid_df$weekday <- wday(UKgrid_df$time, label = TRUE, abbr = TRUE)
UKgrid_df$month <- factor(month.abb[month(UKgrid_df$time)], levels =   month.abb)
head(UKgrid_df)
UKgrid_hourly <- UKgrid_df %>%
dplyr::group_by(hour) %>%
dplyr::summarise(mean = mean(UKgrid, na.rm = TRUE), sd = sd(UKgrid, na.rm
= TRUE))
str(UKgrid_hourly)
UKgrid_hourly
require(plotly)
plot_ly(UKgrid_hourly) %>%
add_lines(x = ~ hour, y = ~ mean, name = "Media") %>%
add_lines(x = ~ hour, y = ~ sd, name = "Desviación Estándar", yaxis =
"y2",
line = list(color = "red", dash = "dash", width = 3)) %>%
layout(
title = "La demanda nacional de electricidad - Promedio horario vs. Desviación Estándar",
yaxis = list(title = "Media"),
yaxis2 = list(overlaying = "y",
side = "right",
title = "Desviación Estándar"
),
xaxis = list(title="Hora del Día"),
legend = list(x = 0.05, y = 0.9),
margin = list(l = 50, r = 50)
)
UKgrid_weekday <- UKgrid_df %>%
dplyr::filter(hour == 3 | hour == 9) %>%
dplyr::group_by(hour, weekday) %>%
dplyr::summarise(mean = mean(UKgrid, na.rm = TRUE),
sd = sd(UKgrid, na.rm = TRUE))
UKgrid_weekday$hour <- factor(UKgrid_weekday$hour)
plot_ly(data = UKgrid_weekday, x = ~ weekday, y = ~ mean, type =
"bar",color = ~ hour) %>%
layout(title = "The Hourly Average Demand by Weekday",
yaxis = list(title = "Mean", range = c(30000, 75000)),
xaxis = list(title = "Weekday"))
UKgrid_month <- UKgrid_df %>%
dplyr::filter(hour == 3 | hour == 9) %>%
dplyr::group_by(hour, month) %>%
dplyr::summarise(mean = mean(UKgrid, na.rm = TRUE),
sd = sd(UKgrid, na.rm = TRUE))
UKgrid_month$hour <- factor(UKgrid_month$hour)
plot_ly(data = UKgrid_month, x = ~ month, y = ~ mean, type = "bar",color =
~ hour) %>%
layout(title = "The Hourly Average Demand by Month",
yaxis = list(title = "Mean", range = c(30000, 75000)),
xaxis = list(title = "Month"))
UKgrid_df$hour<-as.factor(UKgrid_df$hour)
ggplot(UKgrid_df,aes(x=UKgrid))+
geom_density(aes(fill=hour))+
ggtitle("UKgrid-Desidades Kernel estimador para cada hora")+
facet_grid(rows=vars(as.factor(hour)))
UKgrid_df$weekday<-as.factor(UKgrid_df$weekday)
# Se está tomando la hora cero
UKgrid_df%>%dplyr::filter(hour==0)%>%
ggplot(aes(x=UKgrid))+
geom_density(aes(fill=as.factor(weekday)))+
ggtitle("UKgrid-Desidades Kernel estimadas por hora del día")+
facet_grid(rows=vars(as.factor(weekday)))
# Se está tomando la hora 9 a.m
UKgrid_df%>%dplyr::filter(hour==9)%>%
ggplot(aes(x=UKgrid))+
geom_density(aes(fill=as.factor(weekday)))+
ggtitle("UKgrid-Desidades Kernel estimadas por hora del día")+
facet_grid(rows=vars(as.factor(weekday)))
# Se está tomando la hora 9 a.m
UKgrid_df%>%dplyr::filter(hour==17)%>%
ggplot(aes(x=UKgrid))+
geom_density(aes(fill=as.factor(weekday)))+
ggtitle("UKgrid-Desidades Kernel estimadas por hora del día")+
facet_grid(rows=vars(as.factor(weekday)))
ts_quantile(UKgrid,period="weekdays",n=2)
ts_quantile(UKgrid,period="monthly",n=2)
UKgrid_tbl_todas<-as_tibble(UKgrid_df)
UKgrid_tbl_todas<-UKgrid_tbl_todas%>%mutate(UKgrid_trend=smooth_vec(UKgrid,span = 0.75, degree = 2))
UKgrid_tbl_todas%>%ggplot(aes(time, UKgrid)) +
geom_line() +
geom_line(aes(y = UKgrid_trend), color = "red")
UKgrid_tbl_todas<-UKgrid_tbl_todas%>%mutate(UKgrid_no_trend=UKgrid-UKgrid_trend)
colnames(UKgrid_tbl_todas)
UKgrid_tbl_todas%>%ggplot(aes(time, UKgrid_no_trend)) +
geom_line()
knitr::opts_chunk$set(echo = TRUE)
cs = 2*cos(2*pi*1:500/50 + .6*pi);  w = rnorm(500,0,1)
par(mfrow=c(3,1), mar=c(3,2,2,1), cex.main=1.5)
plot.ts(cs, main=expression(2*cos(2*pi*t/50+.6*pi)))
plot.ts(cs+w, main=expression(2*cos(2*pi*t/50+.6*pi) + N(0,1)))
plot.ts(cs+5*w, main=expression(2*cos(2*pi*t/50+.6*pi) + N(0,25)))
cs = 2*cos(2*pi*1:500/50 + .6*pi);  w = rnorm(500,0,1)
par(mfrow=c(3,1), mar=c(3,2,2,1), cex.main=1.5)
plot.ts(cs, main=expression(2*cos(2*pi*t/50+.6*pi)))
plot.ts(cs+w, main=expression(2*cos(2*pi*t/50+.6*pi) + N(0,1)))
plot.ts(cs+5*w, main=expression(2*cos(2*pi*t/50+.6*pi) + N(0,25)))
spectrum(x,log='no')
beta1teor=2*cos(.6*pi)
beta2teor=-2*sin(.6*pi)
set.seed(90210) # fijamos una semilla para que nos dé el mismo resultado
x = 2*cos(2*pi*1:500/50 + .6*pi) + rnorm(500,0,5)
z1 = cos(2*pi*1:500/50)
z2 = sin(2*pi*1:500/50)
summary(fit <- lm(x~0+z1+z2)) # sin intercepto.
beta1teor
beta2teor
spectrum(x,log='no')
spectrum(x,log='no')
abline(v=1/50, lty=2,col="red")
s=4
deltas=seq(1:s)
####Ejemplo Fácil Dummy+ruido
T=200*s
epsilon=rnorm(T,0,1)
l=diag(s)
X=matrix(rep(t(l),T/s),ncol=ncol(l),byrow=TRUE)
Y=X%*%deltas+epsilon
plot(as.ts(Y))
#simul=data.frame(resp=Y,diseno=X)
#salida_dummy_ruido=stan_glm(resp~. -1,data=simul)
PeriodgramaY=spectrum(Y,log='no')
spectrum(as.numeric(dlAirPass),log='no')
spectrum(as.numeric(dlAirPass))
PeriodgramadlAirPass=spectrum(as.numeric(dlAirPass),log='no')
ubicacionlogAir=which.max(PeriodgramadlAirPass$spec)
sprintf("El valor de la frecuencia donde se máximiza el periodograma para la serie es: %s",PeriodgramadlAirPass$freq[ubicacionlogAir])
sprintf("El periodo correspondiente es aproximadamente: %s",1/PeriodgramadlAirPass$freq[ubicacionlogAir])
diff_tsibble<-tsibble_Airpass|>mutate(logdiff_air=difference(log(value)))|>select(logdiff_air)
tsibble_Airpass=as_tsibble(AirPassengers)
diff_tsibble<-tsibble_Airpass|>mutate(logdiff_air=difference(log(value)))|>select(logdiff_air)
diff_tsibble
tsibble_Airpass
###Explore diferentes valores de K
Modelo_serie_diff<-diff_tsibble|>model(
`Fourier1Airdiff`=ARIMA(logdiff_air~fourier(K=2)+pdq(0, 0, 0) + PDQ(0, 0, 0))
)
real_ajustado1<-diff_tsibble%>%left_join(fitted(Modelo_serie_diff,by=index))%>%select(-.model)
real_ajustado1 %>%
autoplot() +
geom_line(data=real_ajustado1,aes(y=logdiff_air,colour="real"))+
geom_line(data=real_ajustado1,aes(y=.fitted,colour="ajustado"))+
scale_color_manual(name = "real/ajustado", values = c("real" = "black", "ajustado" = "red"))
Modelo_serie_diff_Dummy<-diff_tsibble|>model(
`DummyAirdiff`=ARIMA(logdiff_air~season()+pdq(0, 0, 0) + PDQ(0, 0, 0))
)
Modelo_serie_diff_Dummy<-diff_tsibble%>%left_join(fitted(Modelo_serie_diff,by=index))%>%select(-.model)
Modelo_serie_diff_Dummy %>%
autoplot() +
geom_line(data=Modelo_serie_diff_Dummy,aes(y=logdiff_air,colour="real"))+
geom_line(data=Modelo_serie_diff_Dummy,aes(y=.fitted,colour="ajustado"))+
scale_color_manual(name = "real/ajustado", values = c("real" = "black", "ajustado" = "red"))
ajuste_final_models<-diff_tsibble%>%model(
`Fourier1Airdiff`=ARIMA(logdiff_air~fourier(K=1)+pdq(0, 0, 0) + PDQ(0, 0, 0)),
`Fourier2Airdiff`=ARIMA(logdiff_air~fourier(K=2)+pdq(0, 0, 0) + PDQ(0, 0, 0)),
`Fourier3Airdiff`=ARIMA(logdiff_air~fourier(K=3)+pdq(0, 0, 0) + PDQ(0, 0, 0)),
`DummyAirdiff`=ARIMA(logdiff_air~season()+pdq(0, 0, 0) + PDQ(0, 0, 0))
)
glance(ajuste_final_models)
Modelo_serie_diff_models<-diff_tsibble%>%left_join(fitted(ajuste_final_models)|>group_by(.model)%>%
pivot_wider(names_from = .model, values_from = .fitted))
Modelo_serie_diff_models %>%
autoplot() +
geom_line(data=Modelo_serie_diff_models,aes(y=logdiff_air,colour="real"))+
geom_line(data=Modelo_serie_diff_models,aes(y=Fourier1Airdiff,colour="ajustadoFourier1"))+
geom_line(data=Modelo_serie_diff_models,aes(y=Fourier2Airdiff,colour="ajustadoFourier2"))+
geom_line(data=Modelo_serie_diff_models,aes(y=Fourier3Airdiff,colour="ajustadoFourier3"))+
geom_line(data=Modelo_serie_diff_models,aes(y=DummyAirdiff,colour="ajustadoDummy")) +
scale_color_manual(name = "real/ajustado", values = c("real" = "black", "ajustadoFourier1" = "red","ajustadoFourier2" = "blue","ajustadoFourier3"="green","ajustadoDummy"="yellow"))
#na.omit(UKgrid_df$UKgrid)
#which(is.na(UKgrid_df$UKgrid))
UKgridmsts<-msts(UKgrid_df$UKgrid,seasonal.periods = c(24,168,8766))
#na.omit(UKgrid_df$UKgrid)
#which(is.na(UKgrid_df$UKgrid))
UKgridmsts<-msts(UKgrid_df$UKgrid,seasonal.periods = c(24,168,8766))
Ajustes_UKgrid_daily<-tslm(UKgridmsts~fourier(UKgridmsts,K=c(3,0,0)))
summary(Ajustes_UKgrid_daily)
prediction_UKgrid_daily<-predict(Ajustes_UKgrid_daily,data.frame(fourier(UKgridmsts,K=c(3,0,0))))
points_daily<-24*7
plot(UKgridmsts[1:points_daily], ylab="UKgrid",type="l",ylim=c(min(UKgridmsts[1:points_daily],prediction_UKgrid_daily[1:points_daily]),max(UKgridmsts[1:points_daily],prediction_UKgrid_daily[1:points_daily])))
points_daily<-24*7
plot(UKgridmsts[1:points_daily], ylab="UKgrid",type="l",ylim=c(min(UKgridmsts[1:points_daily],prediction_UKgrid_daily[1:points_daily]),max(UKgridmsts[1:points_daily],prediction_UKgrid_daily[1:points_daily])))
lines(seq(1:points_daily),prediction_UKgrid_daily[1:points_daily],col="blue")
Ajustes_UKgrid_weekly<-tslm(UKgridmsts~fourier(UKgridmsts,K=c(0,3,0)))
summary(Ajustes_UKgrid_weekly)
prediction_UKgrid_weekly<-predict(Ajustes_UKgrid_weekly,data.frame(fourier(UKgridmsts,K=c(0,3,0))))
points_weekly<-168*2
plot(UKgridmsts[1:points_weekly], ylab="UKgrid",type="l",ylim=c(min(UKgridmsts[1:points_weekly],prediction_UKgrid_weekly[1:points_weekly]),max(UKgridmsts[1:points_weekly],prediction_UKgrid_weekly[1:points_weekly])))
lines(seq(1:points_weekly),prediction_UKgrid_weekly[1:points_weekly],col="blue")
Ajustes_UKgrid_yearly<-tslm(UKgridmsts~fourier(UKgridmsts,K=c(0,0,3)))
summary(Ajustes_UKgrid_yearly)
prediction_UKgrid_yearly<-predict(Ajustes_UKgrid_yearly,data.frame(fourier(UKgridmsts,K=c(0,0,3))))
points_yearly<-8766
plot(UKgridmsts[1:points_yearly], ylab="UKgrid",type="l",ylim=c(min(UKgridmsts[1:points_yearly],prediction_UKgrid_yearly[1:points_yearly],na.rm =TRUE),max(UKgridmsts[1:points_yearly],prediction_UKgrid_yearly[1:points_yearly],na.rm =TRUE)))
lines(seq(1:points_yearly),prediction_UKgrid_yearly[1:points_yearly],col="blue")
install.packages("Rcmdr")
library(Rcmdr)
library(Rcmdr)
library(Rcmdr)
library(TSA)
library(lmtest)
library(forecast)
library(tseries)
phi1=0.2
phi2=0.48
theta=0.7
Tlength=200
set.seed(123)
y=arima.sim(list(order =c(2,0,1),ar=c(phi1,phi2),ma=c(theta)),n = Tlength)
#x11()
plot(y)
###Búsqueda de p,q vía acf y pacf
acf(y)
###Búsqueda de p,q vía acf y pacf
acf(y)
acf(y,ci.type='ma') ###q máximo 4
pacf(y) ###p máximo 1
######
modelo.propuesto1=forecast::Arima(y,order=c(1,0,0),include.mean = TRUE) ###AR(1)
modelo.propuesto1
lmtest::coeftest(modelo.propuesto1)
####Reestimación quitando la constante
modelo.propuesto1=forecast::Arima(y,order=c(1,0,0),include.mean=FALSE) ###AR(1)
####Se puede usar la función arima de stats o TSA
coeftest(modelo.propuesto1)  ###Todos los parámetros son significativos
AIC(modelo.propuesto1)
BIC(modelo.propuesto1)
modelo.propuesto_arma=forecast::Arima(y,order=c(1,0,4),include.mean=TRUE )
# ,fixed=c(NA,NA,NA,NA,NA)
#,fixed=c(NA,0,NA,0,NA)
coeftest(modelo.propuesto_arma)
modelo.propuesto_arma=forecast::Arima(y,order=c(1,0,4),include.mean=FALSE )
# ,fixed=c(NA,NA,NA,NA,NA)
#,fixed=c(NA,0,NA,0,NA)
coeftest(modelo.propuesto_arma)
modelo.propuesto_arma=forecast::Arima(y,order=c(1,0,4),include.mean=FALSE,fixed=c(NA,NA,NA,0,NA) )
# ,fixed=c(NA,NA,NA,NA,NA)
#,fixed=c(NA,0,NA,0,NA)
coeftest(modelo.propuesto_arma)
modelo.propuesto_arma=forecast::Arima(y,order=c(1,0,4),include.mean=FALSE,fixed=c(NA,0,NA,0,NA) )
# ,fixed=c(NA,NA,NA,NA,NA)
#,fixed=c(NA,0,NA,0,NA)
coeftest(modelo.propuesto_arma)
AIC(modelo.propuesto_arma)
AIC(modelo.propuesto1)
BIC(modelo.propuesto1)
BIC(modelo.propuesto1)
BIC(modelo.propuesto_arma)
##Aquí se calcula la media y la varianza poblacionales del ejemplo----
media1=0.15*0 + 0.35*1 + 0.30*2 + 0.15*3 +0.05*4
media2= 0.20*0+ 0.20*1 + 0.30*2 + 0.15*3 +0.15*4
varianza1=0.15*(0-media1)^2 + 0.35*(1-media1)^2 + 0.30*(2-media1)^2 + 0.15*(3-media1)^2 +0.05*(4-media1)^2
varianza2=0.15*(0-media2)^2 + 0.35*(1-media2)^2 + 0.30*(2-media2)^2 + 0.15*(3-media2)^2 +0.05*(4-media2)^2
###
media1
media2
varianza1
varianza2
1-0.2^2/1.15
1-0.3^2/1.15
(1.5-0.2)^2/2.14
sqrt(0.243889)
sqrt(0.1)
sqrt(0.1*2.14)+0.2
qnorm(0.5)
install.packages("~/Downloads/mtarm_0.1.2.tar.gz", repos = NULL, type = "source")
install.packages("Formula")
install.packages("GIGrvg")
install.packages("coda")
install.packages("~/Downloads/mtarm_0.1.2.tar.gz", repos = NULL, type = "source")
library(mtarm)
data("riverflows")
#Bedon River----
Bedon.fit1.1 <- mtar( ~ Bedon  | Rainfall, row.names=Date, dist="Laplace",
data=riverflows, subset={Date<="2009-04-04" & Date>"2006-01-04"},
ars=list(p=1), n.burnin=1000, n.sim=3000, n.thin=2)
DIC(Bedon.fit1.1)
WAIC(Bedon.fit1.1)
Bedon.fit1.2 <- update(Bedon.fit1.1, ars=list(p=2), subset={Date<="2009-04-04" & Date>"2006-01-03"})
Bedon.fit1.3 <- update(Bedon.fit1.1, ars=list(p=3), subset={Date<="2009-04-04" & Date>"2006-01-02"})
Bedon.fit1.4 <- update(Bedon.fit1.1, ars=list(p=4), subset={Date<="2009-04-04" & Date>"2006-01-01"})
Bedon.fit1.5 <- update(Bedon.fit1.1, ars=list(p=5), subset={Date<="2009-04-04"})
Bedon.fit3.1 <- update(Bedon.fit1.3, ars=list(p=c(1,1,1), d=c(1,1,1)))
Bedon.fit3.2 <- update(Bedon.fit1.3, ars=list(p=c(2,2,2), d=c(2,2,2)))
Bedon.fit3.3 <- update(Bedon.fit1.3, ars=list(p=c(3,3,3), d=c(3,3,3)))
Bedon.fit3.4 <- update(Bedon.fit1.4, ars=list(p=c(4,4,4), d=c(4,4,4)))
Bedon.fit3.5 <- update(Bedon.fit1.5, ars=list(p=c(5,5,5), d=c(5,5,5)))
#Bajo el DIC par el río Bedon tenemos que los mejores modelos
#podrían ser MTAR(3;5,5,5) o MTAR(3;4,4,4) o MTAR(4;4,4,4,4) o MTAR(4;5,5,5,5)
## Resumen del ajuste ----
##Debido a que el modelo MTAR(3;5,5,5) es uno de los modelos(aunque no es el menor) que presenta
###menor WAIC y DIC, tomaremos como base a ese modelo en concordancia con le modelo Bivariado.
summary(Bedon.fit3.5, credible = 0.95)
## Gráfico de Cuantiles ----
plot(Bedon.fit3.5,pch=20,col="black",main="",xlab="Theoretical quantiles",ylab="Sample quantiles")
# Bedon con transformación Logarítmica ----
Bedon.log.fit1.1 <- mtar( ~ Bedon  | Rainfall, row.names=Date, dist="Laplace",
data=riverflows, subset={Date<="2009-04-04" & Date>"2006-01-04"},
ars=list(p=1), n.burnin=1000, n.sim=3000, n.thin=2,log=TRUE)
## Gráfico de Cuantiles ----
plot(Bedon.fit3.5,pch=20,col="black",main="",xlab="Theoretical quantiles",ylab="Sample quantiles")
Bedon.log.fit1.1 <- mtar( ~ Bedon  | Rainfall, row.names=Date, dist="Laplace",
data=riverflows, subset={Date<="2009-04-04" & Date>"2006-01-04"},
ars=list(p=1), n.burnin=1000, n.sim=3000, n.thin=2,log=TRUE)
Bedon.log.fit1.2<-update(Bedon.log.fit1.1,ars=list(p=2),subset={Date<="2009-04-04" & Date>"2006-01-03"})
Bedon.log.fit1.3<-update(Bedon.log.fit1.1,ars=list(p=3),subset={Date<="2009-04-04" & Date>"2006-01-02"})
Bedon.log.fit1.4<-update(Bedon.log.fit1.1,ars=list(p=4),subset={Date<="2009-04-04" & Date>"2006-01-01"})
Bedon.log.fit1.5<-update(Bedon.log.fit1.1,ars=list(p=5),subset={Date<="2009-04-04"})
Bedon.log.fit3.1<-update(Bedon.log.fit1.3,ars=list(p=c(1,1,1),d=c(1,1,1)))
Bedon.log.fit3.2<-update(Bedon.log.fit1.3,ars=list(p=c(2,2,2),d=c(2,2,2)))
Bedon.log.fit3.3<-update(Bedon.log.fit1.3,ars=list(p=c(3,3,3),d=c(3,3,3)))
Bedon.log.fit3.4<-update(Bedon.log.fit1.4,ars=list(p=c(4,4,4),d=c(4,4,4)))
Bedon.log.fit3.5<-update(Bedon.log.fit1.5,ars=list(p=c(5,5,5),d=c(5,5,5)))
WAICs.bedon.log<-cbind(WAIC(Bedon.log.fit1.1,Bedon.log.fit1.2,Bedon.log.fit1.3,Bedon.log.fit1.4,Bedon.log.fit1.5,verbose = FALSE),WAIC(Bedon.log.fit3.1,Bedon.log.fit3.2,Bedon.log.fit3.3,Bedon.log.fit3.4,Bedon.log.fit3.5,verbose = FALSE))
WAICs.bedon.log
pesos=c(303.37, 301.02, 292.24, 300.30, 299.94, 291.24, 297.93, 306.81, 303.49, 302.12,
292.82, 297.89, 296.84, 301.09, 303.95, 289.44, 296.99, 310.30, 296.80, 302.50,
305.99, 301.16, 305.53, 305.02, 303.21, 310.32, 294.25, 299.62, 306.27, 293.37)
hist(pesos,main = "Histograma Pesos Ceibas")
hist(pesos,freq=FALSE,prob=TRUE,col = "lightblue",main='Histograma de pesos de Ceibas',xlab='Toneladas',ylab='Densidad')
lines(density(pesos),col = "chocolate3")
setwd("/Users/sergiocalderonunal/Documents/GitHub/EstadisticaParaGeografosUN/Estadistica_Descriptiva")
library(readxl)
Pesos_Alturas_Diametros <- read_excel("Pesos_Alturas_Diametros.xlsx",
col_types = c("numeric", "numeric", "numeric",
"numeric", "text"))
View(Pesos_Alturas_Diametros)
class(Pesos_Alturas_Diametros)
hist(Pesos_Alturas_Diametros$Pesos)
hist(Pesos_Alturas_Diametros$Alturas)
hist(Pesos_Alturas_Diametros$Diametros)
hist(Pesos_Alturas_Diametros$Pesos)
hist(Pesos_Alturas_Diametros$Alturas)
hist(Pesos_Alturas_Diametros$Diametros)
library(ggplot2)
num_nidos<-c(9,  6,  7,  7,  6,  9,  7,  3,  4,  6,  6,  6,  8,  2,  7,  3,  5,  5,  4,  7,  4,  9,  7,  8,  8,  2, 12, 10,  4,  7)
discreta<-as.data.frame(num_nidos)
ggplot(discreta,aes(num_nidos))+geom_bar()
View(discreta)
hist(num_nidos)
library(arm)
arm::discrete.histogram(discreta$num_nidos)
###
ggplot(Pesos_Alturas_Diametros,aes(Num_Nidos))+geom_bar()
arm::discrete.histogram(Pesos_Alturas_Diametros$Num_Nidos)
dolencias=c("O","O","N","J","C","F","B","B","F","O","J","O","O","O","F","F","O","O",
"N","O","N","J","F","J","B","O","C","J","O","J","J","F","N","O","B","M","O","J","M","O","B",
"O","F",  "J",  "O",  "O",  "B",  "N",  "C",  "O",  "O",  "O","M",  "B","F","J",  "O",  "F",  "N")
Tabladolencias=table(dolencias)
Tabladolencias
barplot(Tabladolencias)
Tabladolencias_prop=Tabladolencias/sum(Tabladolencias)
Tabladolencias_prop
pesos=c(303.37, 301.02, 292.24, 300.30, 299.94, 291.24, 297.93, 306.81, 303.49, 302.12,
292.82, 297.89, 296.84, 301.09, 303.95, 289.44, 296.99, 310.30, 296.80, 302.50,
305.99, 301.16, 305.53, 305.02, 303.21, 310.32, 294.25, 299.62, 306.27, 293.37)
media_pesos<-mean(pesos) #Media
media_pesos
mediana_pesos<-quantile(pesos,probs = 0.5) #Mediana
mediana_pesos
media_pesos
setwd("/Users/sergiocalderonunal/Documents/GitHub/EstadisticaParaGeografosUN/Estadistica_Descriptiva")
readRDS(Base_de_datos_Proyecto.rds)
readRDS("Base_de_datos_Proyecto.rds")
?readRDS
list.files()
readRDS("/Base_de_datos_Proyecto.rds")
list.files()
read_rds("Base_de_datos_Proyecto"  )
readRDS("/Base_de_datos_Proyecto")
getmode <- function(v) {
uniqv <- unique(v)
uniqv[which.max(tabulate(match(v, uniqv)))]
}
moda_pesos<-getmode(pesos) ###Moda
moda_pesos
media_pesos<-mean(pesos) #Media
media_pesos
mediana_pesos<-quantile(pesos,probs = 0.5) #Mediana
mediana_pesos
getmode <- function(v) {
uniqv <- unique(v)
uniqv[which.max(tabulate(match(v, uniqv)))]
}
moda_pesos<-getmode(pesos) ###Moda
moda_pesos
media_diametros<-mean(Pesos_Alturas_Diametros$Diametros)
media_diametros
hist(pesos,col = "white")
abline(v=c(media_pesos,mediana_pesos,moda_pesos),col=c("red","blue","green"))
legend("topleft", legend = c("Media", "Mediana", "Moda"), col = c("red", "blue", "green"), lty = 1, lwd = 2,cex = 0.5)
library(sjstats)
library(moments)
sjstats::cv(pesos) ##Coeficiente de variación es útil, cuando la variable es la misma, pero es hecha en unidades diferentes.
sd(pesos) ###Desviación estándar
sd(pesos)^2 ###Varianza
skewness(pesos) ###Asimetría
hist(pesos)
kurtosis(pesos)###Coeficiente de apuntamiento o curtosis, permite ver si tiene colas mas pesadas que la de la distribución normal(el cual es 3 de forma teórica)
library(readxl)
EncHog2022 <- read_excel("EncHog2022.xlsx",
col_types = c("text", "text", "numeric",
"numeric", "text", "numeric", "numeric",
"numeric"))
View(EncHog2022)
View(EncHog2022)
View(EncHog2022)
Base_EncHog2022%>%filter(Contrato_lab=="1")
library(tidyverse)
Base_EncHog2022%>%filter(Contrato_lab=="1")
View(EncHog2022)
Base_EncHog2022<-read_excel("EncHog2022.xlsx",col_types = c("text", "text", "numeric", "numeric", "text", "numeric", "numeric", "numeric"))
Base_EncHog2022%>%filter(Contrato_lab=="1")
Base_EncHog2022%>%filter(Contrato_lab=="1")%>%select(Gan_Neta)
##Calculo de la media para el grupo de los que si tienen contrato
Base_EncHog2022%>%filter(Contrato_lab=="1")%>%select(Gan_Neta)%>%summarise(media_Gan_Neta = mean(Gan_Neta, na.rm = TRUE))
filtrado_si_contrato_Gan_Neta<-Base_EncHog2022%>%filter(Contrato_lab=="1")%>%select(Gan_Neta)
View(filtrado_si_contrato_Gan_Neta)
hist(filtrado_si_contrato_Gan_Neta)
filtrado_si_contrato_Gan_Neta<-Base_EncHog2022%>%filter(Contrato_lab=="1")%>%select(Gan_Neta)
hist(filtrado_si_contrato_Gan_Neta)
View(filtrado_si_contrato_Gan_Neta)
class(filtrado_si_contrato_Gan_Neta)
hist(filtrado_si_contrato_Gan_Neta$Gan_Neta)
boxplot(filtrado_si_contrato_Gan_Neta$Gan_Neta)
skewness(filtrado_si_contrato_Gan_Neta$Gan_Neta)
kurtosis(filtrado_no_contrato_Gan_Neta$Gan_Neta)
######
filtrado_no_contrato_Gan_Neta<-Base_EncHog2022%>%filter(Contrato_lab=="2")%>%select(Gan_Neta)
kurtosis(filtrado_no_contrato_Gan_Neta$Gan_Neta)
####
hist(Base_EncHog2022$Horas_Trab_Sem)
boxplot(Base_EncHog2022$Horas_Trab_Sem)
mean(Base_EncHog2022$Horas_Trab_Sem)
quantile(Base_EncHog2022$Horas_Trab_Sem,probs = 0.75)
library(readxl)
library(sf)
library(ggplot2)
setwd("/Users/sergiocalderonunal/Documents/GitHub/EstadisticaParaGeografosUN/Estadistica_Descriptiva/Mapas")
Municipios <- readRDS("Municipios.RDS")
Ocupados_ENH <- read_excel("Ocupados_ENH.xlsx",
col_types = c("text", "text", "text",
"numeric", "numeric", "numeric"))
Media_Ing<-Ocupados_ENH%>%group_by(DPTO)%>%summarise(Media_Ing=mean(INGLABO))
Prop_Contrat.1<-Ocupados_ENH%>%group_by(DPTO,P6440)%>%summarise(n=n())%>%
mutate(freq = n / sum(n))%>%filter(P6440=="1")%>%dplyr::select(DPTO,freq)%>%rename(freq.1="freq")
Prop_Contrat.2<-Ocupados_ENH%>%group_by(DPTO,P6440)%>%summarise(n=n())%>%
mutate(freq = n / sum(n))%>%filter(P6440=="2")%>%dplyr::select(DPTO,freq)%>%rename(freq.2="freq")
Media_ING_Prop_Contract<-Media_Ing%>%left_join(Prop_Contrat.1,by=c("DPTO"="DPTO"))%>%left_join(Prop_Contrat.2,by=c("DPTO"="DPTO"))
deptos <- Municipios %>%
group_by(Departamento,Dep) %>%
summarise(Irural=sum(Poblacion*Irural)/sum(Poblacion)) %>%
as.data.frame()
str(deptos)
deptoshp <- st_read("MGN_DPTO_POLITICO.shp",quiet=TRUE)
str(deptoshp)
mapdeptos <- deptoshp %>% left_join(deptos,by=c("DPTO_CCDGO"="Dep"))
str(mapdeptos)
mapdeptos_Ing_Prop <- deptoshp %>% left_join(Media_ING_Prop_Contract,by=c("DPTO_CCDGO"="DPTO"))
str(mapdeptos)
mundoshp <- st_read("admin00.shp",quiet=TRUE)
mundocol <- mundoshp %>%
filter(CNTRY_NAME %in% c("Peru","Brazil","Venezuela","Ecuador","Panama"))
str(mundocol)
help(geom_sf)
box <- st_bbox(mapdeptos)
box
ggplot() +
geom_sf(data=mundocol) +
geom_sf(data=mapdeptos,aes(fill=Irural),col="darkgray",linetype="solid") +
coord_sf(xlim=c(box$xmin,box$xmax),ylim=c(box$ymin,box$ymax),expand=FALSE) +
geom_sf_text(data=mapdeptos,aes(label=ifelse(Irural > 70,Departamento,"")),col="black",
fontface="bold",size=4,fun.geometry=function(x) sf::st_centroid(x)) +
labs(x="Longitud",y="Latitud",title="Colombia",fill="Índice de\nRuralidad") +
scale_fill_gradient(low="white",high="red",n.breaks=5) +
annotate("text", x=c(-74.5,-68,-78,-69,-78.5), y=c(-2.5,0,-1,9,9), colour="blue",
label=c("Perú","Brasil","Ecuador","Venezuela","Panamá")) +
theme(panel.background=element_rect(fill="lightblue"))
ggplot() +
geom_sf(data=mundocol) +
geom_sf(data=mapdeptos_Ing_Prop,aes(fill=Media_Ing),col="darkgray",linetype="solid") +
coord_sf(xlim=c(box$xmin,box$xmax),ylim=c(box$ymin,box$ymax),expand=FALSE) +
geom_sf_text(data=mapdeptos_Ing_Prop,aes(label=ifelse(Media_Ing < 1300000,DPTO_CNMBR,"")),col="black",
fontface="bold",size=4,fun.geometry=function(x) sf::st_centroid(x)) +
labs(x="Longitud",y="Latitud",title="Colombia",fill="Ingreso promedio Laboral") +
scale_fill_gradient(low="white",high="red",n.breaks=5) +
annotate("text", x=c(-74.5,-68,-78,-69,-78.5), y=c(-2.5,0,-1,9,9), colour="blue",
label=c("Perú","Brasil","Ecuador","Venezuela","Panamá")) +
theme(panel.background=element_rect(fill="lightblue"))
####Con Proporción Contrato Laboral ----
ggplot() +
geom_sf(data=mundocol) +
geom_sf(data=mapdeptos_Ing_Prop,aes(fill=freq.1),col="darkgray",linetype="solid") +
coord_sf(xlim=c(box$xmin,box$xmax),ylim=c(box$ymin,box$ymax),expand=FALSE) +
geom_sf_text(data=mapdeptos_Ing_Prop,aes(label=ifelse(freq.1 > 0.5,DPTO_CNMBR,"")),col="black",
fontface="bold",size=2,fun.geometry=function(x) sf::st_centroid(x)) +
labs(x="Longitud",y="Latitud",title="Colombia",fill="Proporción Contrato Laboral") +
scale_fill_gradient(low="white",high="red",n.breaks=5) +
annotate("text", x=c(-74.5,-68,-78,-69,-78.5), y=c(-2.5,0,-1,9,9), colour="blue",
label=c("Perú","Brasil","Ecuador","Venezuela","Panamá")) +
theme(panel.background=element_rect(fill="lightblue"))
